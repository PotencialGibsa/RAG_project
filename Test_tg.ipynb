{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 1,
   "id": "63b0815d-421d-40c4-aa80-d66f60cadb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скачивание документов из папки articles\n",
    "import glob\n",
    "\n",
    "\n",
    "articles_pdf = glob.glob('./articles/*.pdf')\n",
    "articles_docx = glob.glob('./articles/*.doc*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be64cfa6-46e1-4d84-a82a-b1e874794480",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_text = []\n",
    "articles_large_text1 = []\n",
    "articles_large_text2 = []\n",
    "articles_large_text3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f391528-4a62-4ae0-aa51-b2e85171fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделение текста из pdf файлов\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                              chunk_overlap=100)\n",
    "\n",
    "for art_pdf in articles_pdf:\n",
    "    loader = PyPDFLoader(art_pdf)\n",
    "    for chunk_art_pdf in loader.load():\n",
    "        articles_large_text1.append(chunk_art_pdf)\n",
    "    loader = PyPDFLoader(art_pdf)\n",
    "    for chunk_art_pdf in loader.load():\n",
    "        articles_large_text2.append(chunk_art_pdf)\n",
    "    loader = PyPDFLoader(art_pdf)\n",
    "    for chunk_art_pdf in loader.load():\n",
    "        articles_large_text3.append(chunk_art_pdf)\n",
    "    for chunk_art_pdf in loader.load_and_split(text_splitter):\n",
    "        articles_text.append(chunk_art_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f61b41-8de3-4c9a-a17b-dccf5e930216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделение текста из docx файлов\n",
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
    "for art_docx in articles_docx:\n",
    "    loader = UnstructuredWordDocumentLoader(art_docx)\n",
    "    for chunk_art_docx in loader.load():\n",
    "        articles_large_text1.append(chunk_art_docx)\n",
    "    loader = UnstructuredWordDocumentLoader(art_docx)\n",
    "    for chunk_art_docx in loader.load():\n",
    "        articles_large_text2.append(chunk_art_docx)\n",
    "    loader = UnstructuredWordDocumentLoader(art_docx)\n",
    "    for chunk_art_docx in loader.load():\n",
    "        articles_large_text3.append(chunk_art_docx)\n",
    "    for chunk_art_docx in loader.load_and_split(text_splitter):\n",
    "        articles_text.append(chunk_art_docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a9124-5264-4e93-804b-1a2e546a07e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58c4a357-7e89-4a0d-8853-7527f22b143e",
   "metadata": {},
   "source": [
    "## Создадим ансамбль ретриверов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
>>>>>>> Ansemble_retriviers
   "id": "9c500280-62ba-44d3-86eb-db21c05114dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/importlib/__init__.py:126: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n",
<<<<<<< HEAD
      "/tmp/ipykernel_49478/95617426.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=model_name,\n",
=======
      "/tmp/ipykernel_16751/1928784459.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding1 = HuggingFaceEmbeddings(model_name=model_name,\n",
>>>>>>> Ansemble_retriviers
      "/home/dima/RAG_project/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/dima/RAG_project/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embedding1 = HuggingFaceEmbeddings(model_name=model_name,\n",
    "                                  model_kwargs=model_kwargs,\n",
    "                                  encode_kwargs=encode_kwargs)\n",
    "embedding2 = HuggingFaceEmbeddings(model_name=model_name,\n",
    "                                  model_kwargs=model_kwargs,\n",
    "                                  encode_kwargs=encode_kwargs)\n",
    "embedding3 = HuggingFaceEmbeddings(model_name=model_name,\n",
    "                                  model_kwargs=model_kwargs,\n",
    "                                  encode_kwargs=encode_kwargs)\n",
    "\n",
<<<<<<< HEAD
    "vector_store = Chroma('500100',embedding_function = embedding, persist_directory='DataBase/db_500_100')\n",
    "\n",
=======
    "# vector_store = Chroma.from_documents(articles_text, embedding=embedding)\n",
>>>>>>> Ansemble_retriviers
    "\n",
    "# embedding_retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 56,
   "id": "9d1c4260-c1b5-4dba-b9a0-e94cd005ba3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `Австралии` not found.\n",
      "Enswemble output:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'embedder': 'HF_500_100', 'page': 5, 'source': './articles/Нац стратегия.pdf'}, page_content='США.  \\n11. В настоящее время в мире происходит ускоренное внедрение технологических решений, \\nразработанных на основе и скусственного интеллекта, в различные отрасли экономики и сферы \\nобщественных отношений. По оценкам экспертов, ожидается, что благодаря внедрению таких \\nрешений рост мировой экономики в 2024 году составит не менее 1 трлн. долларов США. Указанные \\nтенденции об условлены следующими факторами:  \\nа) общий (\"сквозной\") характер применения прикладных технологических решений,'),\n",
       " Document(metadata={'embedder': 'HF_200_40', 'page': 6, 'source': './articles/Нац стратегия.pdf'}, page_content='для достижения другими участниками рынка конкурентоспособных позиций.  \\n17. Реализация настоящей Стратегии с учетом сложившейся обстано вки на глобальном рынке'),\n",
       " Document(metadata={'source': './articles/Нац стратегия.pdf', 'page': 28, 'embedder': 'HF_1000_200'}, page_content='государстве нных корпораций, государственных компаний, акционерных обществ с \\nгосударственным участием и частные инвестиции. Основными источниками финансового \\nобеспечения реализации настоящей Стратегии являются документы, предусмотренные пунк том \\n54(2)  настоящей Стратегии.  \\n(п. 55 в ред. Указа  Президента РФ от 15.02.2024 N 124)  \\n56. Утратил силу с 15 февраля 2024 года. - Указ  Президента РФ от 15.02.2024 N 124.  \\n57. В целях аналитической поддержки реализации настоящей Стратегии проводятся научные \\nисследования, направленные на прогнозирование развития технологий искусственн ого \\nинтеллекта, а также на прогнозирование социальных и этических аспектов их использования. \\nРезультаты этих исследований должны учитываться при принятии управленческих решений.  \\n58. Утратил силу с 15 февраля 2024 года. - Указ  Президента РФ от 15.02.2024 N 124.  \\n59. Корректировка настоящей Стратегии осуществляется по решению Президента Российской'),\n",
       " Document(metadata={'source': './articles/Нац стратегия.pdf', 'page': 10}, page_content='технологическое отставание.  \\n \\nIII. Основные принципы развития и использования технологий  \\nискусственного интеллекта  \\n \\n19. Основными принципами развития и использования технологий искусственного \\nинтеллекта, соблюдение которых обязательно при реализации настоящей Стратегии, являются:  \\nа) защита прав и свобод человека: обеспечение защиты прав и свобод человека, \\nгарантированных законодательством Российской Федерации, международными договорами'),\n",
       " Document(metadata={'source': './articles/Нац стратегия.pdf', 'page': 2}, page_content='интеллект\" на период до 2030 года;  \\nж) планы мероприятий (\"дорожные карты\") Национальной технологической инициативы;  \\nз) проекты, обеспечивающие достижение целей и показателей деятельности федеральных \\nорганов испо лнительной власти.  \\n(п. 4 в ред. Указа  Президента РФ от 15.02.2024 N 124)  \\n5. Для целей настоящей Стратегии используются следующие основные понятия:  \\nа) искусственный ин теллект - комплекс технологических решений, позволяющий имитировать'),\n",
       " Document(metadata={'source': './articles/Нац стратегия.pdf', 'page': 1}, page_content='(в ред. Указа  Президента РФ от 15.02.2024 N 124)   \\n \\nI. Общие положения  \\n \\n1. Настоящей Стратегией определяются цели и основные задачи развития искусственного \\nинтеллекта в Российской Федерации, а также меры, направленные на его использование в целях \\nобеспечения национальных интересов и реализации стратегических национальных приорите тов, в \\nтом числе в области научно -технологического развития.  \\n2. Правовую основу настоящей Стратегии составляют Конституция  Российской Федерации,')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_core.documents import Document\n",
    "import copy\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "chroma_db_750_150 = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embedding1,\n",
    ")\n",
    "\n",
    "text_splitter1 = RecursiveCharacterTextSplitter(chunk_size=750,\n",
    "                                            chunk_overlap=150)\n",
    "chunks_ret = []\n",
    "for alt in copy.deepcopy(articles_large_text1):\n",
    "    alt.metadata['embedder'] = 'HF_750_150'\n",
    "    chunks_ret.extend(text_splitter1.split_documents([alt]))\n",
    "chroma_db_750_150.add_documents(chunks_ret.copy())\n",
    "embedding_retriever_750_150 = chroma_db_750_150.as_retriever(search_kwargs={\"k\": 1})\n",
    "#print('embedding_retriever_200_40', embedding_retriever_200_40.invoke('Расскажи про основные показатели цаниональной стратегии?'))\n",
    "\n",
    "text_splitter2 = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                            chunk_overlap=100)\n",
    "chunks_ret = []\n",
    "for alt in copy.deepcopy(articles_large_text2):\n",
    "    alt.metadata['embedder'] = 'HF_500_100'\n",
    "    chunks_ret.extend(text_splitter2.split_documents([alt]))\n",
    "chroma_db_500_100 = Chroma.from_documents(chunks_ret.copy(), embedding=embedding2)\n",
    "embedding_retriever_500_100 = chroma_db_500_100.as_retriever(search_kwargs={\"k\": 1})\n",
    "#print('embedding_retriever_500_100', embedding_retriever_500_100.invoke('Расскажи про основные показатели цаниональной стратегии?'))\n",
    "\n",
    "text_splitter3 = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "                                            chunk_overlap=200)\n",
    "chunks_ret = []\n",
    "for alt in copy.deepcopy(articles_large_text3):\n",
    "    alt.metadata['embedder'] = 'HF_1000_200'\n",
    "    chunks_ret.extend(text_splitter3.split_documents([alt]))\n",
    "faiss_db_1000_200 = FAISS.from_documents(chunks_ret.copy(), embedding=embedding3)\n",
    "embedding_retriever_1000_200 = faiss_db_1000_200.as_retriever(search_kwargs={\"k\": 1})\n",
    "#print('embedding_retriever_1000_200', embedding_retriever_1000_200.invoke('Расскажи про основные показатели цаниональной стратегии?'))\n",
    "\n",
    "\n",
    "# chroma_db_2000_400 = Chroma(\n",
    "#     collection_name=\"2000_400\",\n",
    "#     embedding_function=embedding1,\n",
    "# )\n",
    "\n",
    "# text_splitter4 = RecursiveCharacterTextSplitter(chunk_size=2000,\n",
    "#                                             chunk_overlap=400)\n",
    "# chunks_ret = []\n",
    "# for alt in copy.deepcopy(articles_large_text1):\n",
    "#     alt.metadata['embedder'] = 'HF_2000_400'\n",
    "#     chunks_ret.extend(text_splitter4.split_documents([alt]))\n",
    "# chroma_db_2000_400.add_documents(chunks_ret.copy())\n",
    "# embedding_retriever_2000_400 = chroma_db_2000_400.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    return s.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).split(\" \")\n",
    "\n",
    "Какой процент экономики планируется заменить искусственным интеллектом в Австралии?\n",
    "bm25_retriever = BM25Retriever.from_documents(\n",
    "    documents=articles_text,\n",
    "    #metadatas= {\"embedder\": 'bm25'}, #* len(articles_text),\n",
    "    preprocess_func=tokenize,\n",
    "    k=3,\n",
    ")\n",
    "     \n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[\n",
    "            #embedding_retriever_200_40,\n",
    "            embedding_retriever_500_100,\n",
    "            embedding_retriever_750_150,\n",
    "            embedding_retriever_1000_200,\n",
    "            #embedding_retriever_2000_400,\n",
    "            bm25_retriever\n",
    "    ],\n",
    "    weights=[1/4 , 1/4, 1/4, 1/4],\n",
    "    #weights=[1/2, 1/2],\n",
    ")\n",
    "print('Enswemble output:')\n",
    "ensemble_retriever.invoke('Расскажи про основные показатели цаниональной стратегии?')\n",
    "#qa.invoke({\"query\": q1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9871af-d50f-4e1c-a8b9-d27997291009",
   "metadata": {},
   "source": [
    "## Сначала HF embeddings, на выбранных файлах bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e073b088-4503-4bc1-a8b1-441cd2f994d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d300a8e4-7d34-4818-a006-87b6dc000159",
   "metadata": {},
   "source": [
    "## Создание промпта и инициализация Gigachat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
>>>>>>> Ansemble_retriviers
   "id": "0e1c4fdf-8675-4053-9e9b-cd9d4e9865cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/RAG_project/lib/python3.10/site-packages/pydantic/_internal/_config.py:334: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models.gigachat import GigaChat\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "credentials = 'Y2U1MzUwNTMtZGU4Ny00ZDNlLWE5OTEtNmQ3YTQ4NWMwYWU0OmQwOWUxZTViLTliOGMtNDQ4ZC1iN2NlLWYzZmE0ZjVjNzA4OA=='\n",
    "\n",
    "llm = GigaChat(credentials=credentials,\n",
    "              model='GigaChat:latest',\n",
    "               verify_ssl_certs=False,\n",
    "               profanity_check=False)\n",
    "\n",
    "\n",
    "prompt_question = ChatPromptTemplate.from_template('''Твоя задача: Переформулировать вопрос пользователя в соответствии с \\\n",
    "данным контекстом и предыдущей историей диалога, сохранив изначальный смысл вопроса. Если необходимо добавляй в вопрос уточняющие детали.\n",
    "\\\n",
    "################################\n",
    "Данные для формирования ответа:\\\n",
    "    Контекст: {context}\n",
    "    История диалога: {history}\n",
    "################################\n",
    "Вопрос пользователя:\n",
    "    {input} \\\n",
    "################################\n",
    "Твоя формулировка вопроса:\\\n",
    "    Ответ:'''                                                             )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''Твоя цель: развернуто отвечать на задаваемые вопросы. \\\n",
    "Ты можешь использовать для ответа только информацию из контекста и из истории диалога.\\\n",
    "Если в контексте или истории диалога нет информации для ответа, проси уточнить вопрос.\\\n",
    "\\\n",
    "################################\n",
    "Данные для формирования ответа:\\\n",
    "    Контекст: {context} \\\n",
    "    История диалога :{history} \\ \n",
    "################################\n",
    "Вопрос пользователя:\n",
    "    {input} \\\n",
    "################################\n",
    "Твой ответ:\\\n",
    "    Ответ:'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 58,
>>>>>>> Ansemble_retriviers
   "id": "045bc9a2-4f7e-46ae-b997-263874f1bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    "    )\n",
    "question_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_question\n",
    "    )\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(ensemble_retriever, document_chain)\n",
    "retrieval_question_chain = create_retrieval_chain(ensemble_retriever, question_chain)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 59,
>>>>>>> Ansemble_retriviers
   "id": "8dda8b83-1138-4db9-9417-333c4f9a6ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import telebot\n",
    "from time import sleep\n",
    "bot_token = '7617208111:AAG8owpHHYyj3U4biEbwJ6NU1J3PB61WRtI'\n",
    "bot = telebot.TeleBot(bot_token)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 60,
>>>>>>> Ansemble_retriviers
   "id": "d15cd6c5-1fa2-44c5-ace2-6ea9dd32dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from operator import itemgetter\n",
    "from collections import deque\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import BaseMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    ConfigurableFieldSpec,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"In memory implementation of chat message history.\"\"\"\n",
    "\n",
    "    messages = deque()\n",
    "\n",
    "    def add_messages(self, messages):\n",
    "        \"\"\"Add a list of messages to the store\"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        if len(self.messages) > 10:\n",
    "            while len(self.messages) > 10:\n",
    "                self.messages.popleft()\n",
    "\n",
    "    def clear(self):\n",
    "        self.messages = deque()\n",
    "\n",
    "# Here we use a global variable to store the chat message history.\n",
    "# This will make it easier to inspect it to see the underlying results.\n",
    "store = {}\n",
    "\n",
    "def get_by_session_id(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "# history = get_by_session_id(\"1\")\n",
    "# history.add_message(AIMessage(content=\"hello\"))\n",
    "# print(store) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e26ab555-2b82-4207-800e-14c730aa2c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "input_variables=['context', 'history', 'input'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'history', 'input'], input_types={}, partial_variables={}, template='Твоя цель: отвечать на задаваемые вопросы. Ты можешь использовать для ответа только информацию из контекста и из истории диалога.Если в контексте или истории диалога нет информации для ответа, проси уточнить вопрос.################################\\nДанные для формирования ответа:    Контекст: {context}     История диалога :{history} \\\\ \\n################################\\nВопрос пользователя:\\n    {input} ################################\\nТвой ответ:    Ответ:'), additional_kwargs={})]\n",
      "input_variables=['context', 'history', 'input'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'history', 'input'], input_types={}, partial_variables={}, template='Твоя цель: отвечать на задаваемые вопросы. Ты можешь использовать для ответа только информацию из контекста и из истории диалога.Если в контексте или истории диалога нет информации для ответа, проси уточнить вопрос.################################\\nДанные для формирования ответа:    Контекст: {context}     История диалога :{history} \\\\ \\n################################\\nВопрос пользователя:\\n    {input} ################################\\nТвой ответ:    Ответ:'), additional_kwargs={})]\n",
      "input_variables=['context', 'history', 'input'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'history', 'input'], input_types={}, partial_variables={}, template='Твоя цель: отвечать на задаваемые вопросы. Ты можешь использовать для ответа только информацию из контекста и из истории диалога.Если в контексте или истории диалога нет информации для ответа, проси уточнить вопрос.################################\\nДанные для формирования ответа:    Контекст: {context}     История диалога :{history} \\\\ \\n################################\\nВопрос пользователя:\\n    {input} ################################\\nТвой ответ:    Ответ:'), additional_kwargs={})]\n",
      "input_variables=['context', 'history', 'input'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'history', 'input'], input_types={}, partial_variables={}, template='Твоя цель: отвечать на задаваемые вопросы. Ты можешь использовать для ответа только информацию из контекста и из истории диалога.Если в контексте или истории диалога нет информации для ответа, проси уточнить вопрос.################################\\nДанные для формирования ответа:    Контекст: {context}     История диалога :{history} \\\\ \\n################################\\nВопрос пользователя:\\n    {input} ################################\\nТвой ответ:    Ответ:'), additional_kwargs={})]\n",
      "input_variables=['context', 'history', 'input'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'history', 'input'], input_types={}, partial_variables={}, template='Твоя цель: отвечать на задаваемые вопросы. Ты можешь использовать для ответа только информацию из контекста и из истории диалога.Если в контексте или истории диалога нет информации для ответа, проси уточнить вопрос.################################\\nДанные для формирования ответа:    Контекст: {context}     История диалога :{history} \\\\ \\n################################\\nВопрос пользователя:\\n    {input} ################################\\nТвой ответ:    Ответ:'), additional_kwargs={})]\n",
      "input_variables=['context', 'history', 'input'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'history', 'input'], input_types={}, partial_variables={}, template='Твоя цель: отвечать на задаваемые вопросы. Ты можешь использовать для ответа только информацию из контекста и из истории диалога.Если в контексте или истории диалога нет информации для ответа, проси уточнить вопрос.################################\\nДанные для формирования ответа:    Контекст: {context}     История диалога :{history} \\\\ \\n################################\\nВопрос пользователя:\\n    {input} ################################\\nТвой ответ:    Ответ:'), additional_kwargs={})]\n",
      "input_variables=['context', 'history', 'input'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'history', 'input'], input_types={}, partial_variables={}, template='Твоя цель: отвечать на задаваемые вопросы. Ты можешь использовать для ответа только информацию из контекста и из истории диалога.Если в контексте или истории диалога нет информации для ответа, проси уточнить вопрос.################################\\nДанные для формирования ответа:    Контекст: {context}     История диалога :{history} \\\\ \\n################################\\nВопрос пользователя:\\n    {input} ################################\\nТвой ответ:    Ответ:'), additional_kwargs={})]\n"
=======
      "input_variables=['context', 'history', 'input'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'history', 'input'], input_types={}, partial_variables={}, template='Твоя цель: развернуто отвечать на задаваемые вопросы. Ты можешь использовать для ответа только информацию из контекста и из истории диалога.Если в контексте или истории диалога нет информации для ответа, проси уточнить вопрос.################################\\nДанные для формирования ответа:    Контекст: {context}     История диалога :{history} \\\\ \\n################################\\nВопрос пользователя:\\n    {input} ################################\\nТвой ответ:    Ответ:'), additional_kwargs={})]\n",
      "input_variables=['context', 'history', 'input'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'history', 'input'], input_types={}, partial_variables={}, template='Твоя цель: развернуто отвечать на задаваемые вопросы. Ты можешь использовать для ответа только информацию из контекста и из истории диалога.Если в контексте или истории диалога нет информации для ответа, проси уточнить вопрос.################################\\nДанные для формирования ответа:    Контекст: {context}     История диалога :{history} \\\\ \\n################################\\nВопрос пользователя:\\n    {input} ################################\\nТвой ответ:    Ответ:'), additional_kwargs={})]\n",
      "input_variables=['context', 'history', 'input'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'history', 'input'], input_types={}, partial_variables={}, template='Твоя цель: развернуто отвечать на задаваемые вопросы. Ты можешь использовать для ответа только информацию из контекста и из истории диалога.Если в контексте или истории диалога нет информации для ответа, проси уточнить вопрос.################################\\nДанные для формирования ответа:    Контекст: {context}     История диалога :{history} \\\\ \\n################################\\nВопрос пользователя:\\n    {input} ################################\\nТвой ответ:    Ответ:'), additional_kwargs={})]\n",
      "input_variables=['context', 'history', 'input'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'history', 'input'], input_types={}, partial_variables={}, template='Твоя цель: развернуто отвечать на задаваемые вопросы. Ты можешь использовать для ответа только информацию из контекста и из истории диалога.Если в контексте или истории диалога нет информации для ответа, проси уточнить вопрос.################################\\nДанные для формирования ответа:    Контекст: {context}     История диалога :{history} \\\\ \\n################################\\nВопрос пользователя:\\n    {input} ################################\\nТвой ответ:    Ответ:'), additional_kwargs={})]\n",
      "input_variables=['context', 'history', 'input'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'history', 'input'], input_types={}, partial_variables={}, template='Твоя цель: развернуто отвечать на задаваемые вопросы. Ты можешь использовать для ответа только информацию из контекста и из истории диалога.Если в контексте или истории диалога нет информации для ответа, проси уточнить вопрос.################################\\nДанные для формирования ответа:    Контекст: {context}     История диалога :{history} \\\\ \\n################################\\nВопрос пользователя:\\n    {input} ################################\\nТвой ответ:    Ответ:'), additional_kwargs={})]\n",
      "input_variables=['context', 'history', 'input'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'history', 'input'], input_types={}, partial_variables={}, template='Твоя цель: развернуто отвечать на задаваемые вопросы. Ты можешь использовать для ответа только информацию из контекста и из истории диалога.Если в контексте или истории диалога нет информации для ответа, проси уточнить вопрос.################################\\nДанные для формирования ответа:    Контекст: {context}     История диалога :{history} \\\\ \\n################################\\nВопрос пользователя:\\n    {input} ################################\\nТвой ответ:    Ответ:'), additional_kwargs={})]\n",
      "input_variables=['context', 'history', 'input'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'history', 'input'], input_types={}, partial_variables={}, template='Твоя цель: развернуто отвечать на задаваемые вопросы. Ты можешь использовать для ответа только информацию из контекста и из истории диалога.Если в контексте или истории диалога нет информации для ответа, проси уточнить вопрос.################################\\nДанные для формирования ответа:    Контекст: {context}     История диалога :{history} \\\\ \\n################################\\nВопрос пользователя:\\n    {input} ################################\\nТвой ответ:    Ответ:'), additional_kwargs={})]\n"
>>>>>>> Ansemble_retriviers
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "users_activated = {}\n",
    "\n",
    "# Функция для автоматического ответа в случае нетекстового сообщения\n",
    "@bot.message_handler(content_types=['audio',\n",
    "                                    'video',\n",
    "                                    'document',\n",
    "                                    'photo',\n",
    "                                    'sticker',\n",
    "                                    'voice',\n",
    "                                    'location',\n",
    "                                    'contact'])\n",
    "def not_text(message):\n",
    "  user_id = message.chat.id\n",
    "  bot.send_message(user_id, 'Я работаю только с текстовыми сообщениями!')\n",
    "\n",
    "\n",
    "def ask_bot(retrieval_chain, chain_question_with_history, question, user_id):\n",
    "    config={\"configurable\": {\"session_id\": str(user_id)}}\n",
    "    quest = chain_question_with_history.invoke(\n",
    "        {'input': question}, config\n",
    "    )['answer']\n",
    "    resp = retrieval_chain.invoke(\n",
    "        {'input': question}, config\n",
    "    )\n",
    "    print(prompt)\n",
    "    return quest, resp['answer']\n",
    "    \n",
    "# Функция, обрабатывающая текстовые сообщения\n",
    "@bot.message_handler(content_types=['text'])\n",
    "def handle_text_message(message):\n",
    "    user_id = message.chat.id\n",
    "    if users_activated.get(user_id, False):\n",
    "        chain_with_history = RunnableWithMessageHistory(\n",
    "            retrieval_chain,\n",
    "            # Uses the get_by_session_id function defined in the example\n",
    "            # above.\n",
    "            get_by_session_id,\n",
    "            input_messages_key=\"input\",\n",
    "            history_messages_key=\"history\",\n",
    "            output_messages_key = 'answer'\n",
    "        )\n",
    "        chain_question_with_history = RunnableWithMessageHistory(\n",
    "            retrieval_question_chain,\n",
    "            # Uses the get_by_session_id function defined in the example\n",
    "            # above.\n",
    "            get_by_session_id,\n",
    "            input_messages_key=\"input\",\n",
    "            history_messages_key=\"history\",\n",
    "            output_messages_key = 'answer'\n",
    "        )\n",
    "        question = message.text\n",
    "        quest_made, answer = ask_bot(chain_with_history, chain_question_with_history, question, user_id)\n",
    "        # resp1 = retrieval_chain.invoke(\n",
    "        #     {'input': q1}\n",
    "        # )\n",
    "        \n",
    "        bot.send_message(user_id, text = f'This is the question:    {quest_made}\\n\\nThis is the answer:     {answer}')#user_id, resp1['answer'])\n",
    "        sleep(2)\n",
    "    else:\n",
    "        users_activated[user_id] = True\n",
    "        bot.send_message(user_id, text = \"Glad to see you! Ask your questions\")\n",
    "        sleep(2)\n",
    "\n",
    "# Запуск бота\n",
    "bot.polling(none_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b17823ee-132f-47ed-b772-169fe995d021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'history', 'input'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'history', 'input'], input_types={}, partial_variables={}, template='Твоя цель: отвечать на задаваемые вопросы. Ты можешь использовать для ответа только информацию из контекста и из истории диалога.Если в контексте или истории диалога нет информации для ответа, проси уточнить вопрос.################################\\nДанные для формирования ответа:    Контекст: {context}     История диалога :{history} \\\\ \\n################################\\nВопрос пользователя:\\n    {input} ################################\\nТвой ответ:    Ответ:'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3320d4-2d03-43d0-b3fe-6df5f0763321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_project",
   "language": "python",
   "name": "rag_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
